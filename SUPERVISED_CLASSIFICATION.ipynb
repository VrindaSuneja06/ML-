{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**THEORY**"
      ],
      "metadata": {
        "id": "SHh1PlMyrpDL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Information Gain, and how is it used in Decision Trees?\n",
        "Answer:\n",
        "Information Gain is a measure used in Decision Trees to determine which feature should be selected for splitting the data at each node. It is based on the concept of entropy, which represents the amount of uncertainty or impurity present in a dataset. When a dataset is split using a feature, the entropy of the resulting subsets is calculated. Information Gain is the difference between the entropy before the split and the weighted entropy after the split.\n",
        "In Decision Trees, the feature that provides the highest Information Gain is chosen because it results in the most significant reduction in uncertainty. This helps the model create purer child nodes and improves classification accuracy.\n",
        "\n",
        "Question 2: What is the difference between Gini Impurity and Entropy?\n",
        "Answer:\n",
        "Gini Impurity and Entropy are both used to measure impurity in a dataset while constructing Decision Trees, but they differ in calculation and interpretation. Gini Impurity measures the probability that a randomly selected data point would be incorrectly classified if it were randomly labeled according to the class distribution. It is computationally efficient and faster because it avoids logarithmic operations.\n",
        "Entropy measures the amount of randomness or disorder in a dataset using logarithmic calculations. It is more sensitive to changes in class distribution and provides a more detailed measure of impurity. Due to this sensitivity, Entropy may sometimes result in more balanced splits, but it is computationally more expensive.\n",
        "\n",
        "In practice, Gini Impurity is commonly preferred for large datasets due to its speed, while Entropy is used when a more precise measurement of impurity is required. Both usually produce similar results in terms of model accuracy.\n",
        "\n",
        "Question 3: What is Pre-Pruning in Decision Trees?\n",
        "Answer:\n",
        "Pre-pruning is a technique used to control the growth of a Decision Tree during the training phase itself. It prevents the tree from becoming too complex by setting stopping conditions such as maximum tree depth, minimum number of samples required to split a node, or minimum samples required at a leaf node.\n",
        "\n",
        "The main purpose of pre-pruning is to avoid overfitting, where the tree learns noise from the training data instead of meaningful patterns. By limiting tree growth early, pre-pruning improves model generalization and reduces computation time, although excessive pruning may lead to underfitting.\n",
        "\n",
        "Question 5: What is a Support Vector Machine (SVM)?\n",
        "Answer:\n",
        "A Support Vector Machine is a supervised machine learning algorithm used for classification and regression tasks. It works by finding an optimal hyperplane that separates data points of different classes with the maximum possible margin. The data points that lie closest to the hyperplane are known as support vectors and play a crucial role in defining the decision boundary.\n",
        "SVM is effective in high-dimensional spaces and performs well even when the number of features is large compared to the number of samples.\n",
        "\n",
        "Question 6: What is the Kernel Trick in SVM?\n",
        "Answer:\n",
        "The Kernel Trick is a technique used in Support Vector Machines to handle non-linearly separable data. It works by implicitly transforming the input data into a higher-dimensional feature space where a linear separation becomes possible. This transformation is done without explicitly computing the coordinates in the higher dimension, which makes the process computationally efficient.\n",
        "\n",
        "Common kernel functions include linear, polynomial, and radial basis function (RBF) kernels.\n",
        "\n",
        "Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "Answer:\n",
        "Naïve Bayes is a probabilistic classification algorithm based on Bayes’ Theorem. It calculates the probability of a data point belonging to a particular class by assuming that all features are conditionally independent given the class label.\n",
        "It is called \"Naïve\" because this assumption of feature independence is rarely true in real-world data. Despite this simplification, Naïve Bayes performs efficiently and accurately in many applications such as text classification and spam filtering.\n",
        "\n",
        "Question 9: Explain the differences between Gaussian, Multinomial, and Bernoulli Naïve Bayes\n",
        "Answer:\n",
        "Gaussian Naïve Bayes is used when the input features are continuous and assumed to follow a normal distribution. It is commonly applied in medical and scientific datasets. Multinomial Naïve Bayes is suitable for discrete data, especially count-based features such as word frequencies in text classification problems. Bernoulli Naïve Bayes is designed for binary features where values represent the presence or absence of a feature, making it useful for binary text data."
      ],
      "metadata": {
        "id": "XcgS69e1rt1a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PRACTICAL**"
      ],
      "metadata": {
        "id": "H2laRtBVr2oe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#4\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load the dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train Decision Tree using Gini Impurity\n",
        "dt_model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "dt_model.fit(X, y)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for name, importance in zip(data.feature_names, dt_model.feature_importances_):\n",
        "    print(name, \":\", importance)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sKd8zf1r4s0",
        "outputId": "8600e6e6-8976-4a8f-9dda-d69bd096336c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "sepal length (cm) : 0.013333333333333329\n",
            "sepal width (cm) : 0.0\n",
            "petal length (cm) : 0.5640559581320451\n",
            "petal width (cm) : 0.4226107085346215\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Linear Kernel SVM\n",
        "linear_svm = SVC(kernel='linear')\n",
        "linear_svm.fit(X_train, y_train)\n",
        "linear_predictions = linear_svm.predict(X_test)\n",
        "linear_accuracy = accuracy_score(y_test, linear_predictions)\n",
        "\n",
        "# RBF Kernel SVM\n",
        "rbf_svm = SVC(kernel='rbf')\n",
        "rbf_svm.fit(X_train, y_train)\n",
        "rbf_predictions = rbf_svm.predict(X_test)\n",
        "rbf_accuracy = accuracy_score(y_test, rbf_predictions)\n",
        "\n",
        "print(\"Linear SVM Accuracy:\", linear_accuracy)\n",
        "print(\"RBF SVM Accuracy:\", rbf_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfA23-eHsLSi",
        "outputId": "657951e5-96be-4660-81cd-31ed17cc201e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear SVM Accuracy: 0.9814814814814815\n",
            "RBF SVM Accuracy: 0.7592592592592593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#10\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train Gaussian Naive Bayes model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate accuracy\n",
        "y_pred = gnb.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Model Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-RYEM5ZscW2",
        "outputId": "a60a28a0-13df-4510-ecc9-fc296ab70b93"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.9415204678362573\n"
          ]
        }
      ]
    }
  ]
}