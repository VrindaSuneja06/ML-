{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**THEORY**"
      ],
      "metadata": {
        "id": "-w5cG-0D44RU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1\n",
        "What is a Decision Tree, and how does it work in the context of classification?\n",
        "\n",
        "Answer:\n",
        "\n",
        "A Decision Tree is a supervised machine learning algorithm used for classification and regression. In classification, it works by recursively splitting the dataset based on feature values so that the resulting subsets are as pure as possible.\n",
        "\n",
        "Each internal node represents a decision on a feature, each branch represents an outcome of that decision, and each leaf node represents a class label. The tree selects the best splits using impurity measures such as Gini Impurity or Entropy.\n",
        "\n",
        "Question 2\n",
        "Explain Gini Impurity and Entropy as impurity measures. How do they impact splits?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Gini Impurity measures the probability of incorrect classification of a randomly chosen sample.\n",
        "\n",
        "Formula:\n",
        "Gini = 1 − Σ(pi²)\n",
        "\n",
        "Entropy measures the level of randomness or uncertainty in the data.\n",
        "\n",
        "Formula:\n",
        "Entropy = − Σ(pi log₂ pi)\n",
        "\n",
        "Lower impurity values indicate purer nodes. During tree construction, the algorithm chooses the split that minimizes impurity (or maximizes information gain), leading to better class separation.\n",
        "\n",
        "Question 3\n",
        "Difference between Pre-Pruning and Post-Pruning with one advantage each.\n",
        "\n",
        "Answer:\n",
        "\n",
        "Pre-Pruning stops the tree from growing early by applying constraints such as maximum depth or minimum samples per split.\n",
        "Advantage: Reduces overfitting and improves generalization.\n",
        "\n",
        "Post-Pruning grows a complete tree and then removes unnecessary branches.\n",
        "Advantage: Produces a more optimized tree after observing the full structure.\n",
        "\n",
        "Question 4\n",
        "What is Information Gain and why is it important?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Information Gain measures the reduction in entropy after splitting a dataset on a feature.\n",
        "\n",
        "Information Gain = Entropy(parent) − Weighted Entropy(children)\n",
        "\n",
        "It is important because it helps select the feature that best separates the classes, leading to more accurate and efficient decision trees.\n",
        "\n",
        "Question 5\n",
        "Real-world applications, advantages, and limitations of Decision Trees.\n",
        "\n",
        "Answer:\n",
        "\n",
        "Applications:\n",
        "\n",
        "Medical diagnosis\n",
        "\n",
        "Credit risk assessment\n",
        "\n",
        "Fraud detection\n",
        "\n",
        "Customer segmentation\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Easy to understand and interpret\n",
        "\n",
        "Handles numerical and categorical data\n",
        "\n",
        "Requires minimal data preprocessing\n",
        "\n",
        "Limitations:\n",
        "\n",
        "Prone to overfitting\n",
        "\n",
        "Sensitive to noisy data\n",
        "\n",
        "Small changes in data can lead to different trees"
      ],
      "metadata": {
        "id": "2cFLgOHZ48eY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PRACTICAL**"
      ],
      "metadata": {
        "id": "HAHGzvuM5MH5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd0xp9Eb42Ld",
        "outputId": "1d19834e-14be-4ba0-cf2b-3deffba3993d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Feature Importances: [0.         0.01667014 0.90614339 0.07718647]\n"
          ]
        }
      ],
      "source": [
        "#6\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Feature Importances:\", model.feature_importances_)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7\n",
        "full_tree = DecisionTreeClassifier(random_state=42)\n",
        "full_tree.fit(X_train, y_train)\n",
        "\n",
        "pruned_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "pruned_tree.fit(X_train, y_train)\n",
        "\n",
        "print(\"Fully Grown Tree Accuracy:\", full_tree.score(X_test, y_test))\n",
        "print(\"Pruned Tree Accuracy:\", pruned_tree.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_v8JbB5A5fDC",
        "outputId": "5fc09dc8-2b96-40e5-e85c-f79001edee8b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fully Grown Tree Accuracy: 1.0\n",
            "Pruned Tree Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "reg = DecisionTreeRegressor(random_state=42)\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
        "print(\"Feature Importances:\", reg.feature_importances_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rjr3xeI15roP",
        "outputId": "facd2ca9-5b67-4eca-fe51-685421b5b740"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.495235205629094\n",
            "Feature Importances: [0.52850909 0.05188354 0.05297497 0.02866046 0.03051568 0.13083768\n",
            " 0.09371656 0.08290203]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Parameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 7, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Decision Tree model\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# GridSearchCV\n",
        "grid = GridSearchCV(dt, param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Best model evaluation\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Cross-Validation Accuracy:\", grid.best_score_)\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEUhuhqY5y3O",
        "outputId": "485dd96f-8779-4665-f800-5ef79ee622f2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 7, 'min_samples_split': 2}\n",
            "Best Cross-Validation Accuracy: 0.9416666666666668\n",
            "Test Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10\n",
        "Healthcare use case: step-by-step process and business value\n",
        "\n",
        "Answer:\n",
        "\n",
        "Handling missing values:\n",
        "\n",
        "Use mean or median for numerical features\n",
        "\n",
        "Use mode for categorical features\n",
        "\n",
        "Encoding categorical features:\n",
        "\n",
        "Apply one-hot encoding or label encoding\n",
        "\n",
        "Training the model:\n",
        "\n",
        "Train a Decision Tree classifier on the cleaned dataset\n",
        "\n",
        "Hyperparameter tuning:\n",
        "\n",
        "Tune max_depth, min_samples_split, and min_samples_leaf using GridSearchCV\n",
        "\n",
        "Model evaluation:\n",
        "\n",
        "Use accuracy, precision, recall, F1-score, and confusion matrix\n",
        "\n",
        "Business value:\n",
        "This model can support early disease detection, reduce diagnostic errors, assist doctors in decision-making, improve patient outcomes, and lower healthcare costs."
      ],
      "metadata": {
        "id": "JKJ9hzqv57Yv"
      }
    }
  ]
}