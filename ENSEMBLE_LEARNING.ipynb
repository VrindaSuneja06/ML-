{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**THEORY QUESTIONS**"
      ],
      "metadata": {
        "id": "naqyqLxbvKup"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Can we use Bagging for regression problems?\n",
        "\n",
        "Yes, Bagging can be used for regression problems. In Bagging Regression, multiple regression models are trained on different bootstrap samples of the dataset. The final prediction is obtained by averaging the predictions of all models. This reduces variance and improves prediction stability, especially for high-variance models like Decision Trees.\n",
        "\n",
        "2. What is the difference between multiple model training and single model training?\n",
        "\n",
        "Single model training involves training one model on the entire dataset, which may suffer from overfitting or high variance. Multiple model training involves training several models and combining their predictions, which improves generalization, reduces variance, and increases robustness.\n",
        "\n",
        "3. Explain the concept of feature randomness in Random Forest\n",
        "\n",
        "In Random Forest, feature randomness means that at each split in a tree, only a random subset of features is considered. This reduces correlation between trees and ensures diversity in the ensemble, which improves overall performance.\n",
        "\n",
        "4. What is OOB (Out-of-Bag) Score?\n",
        "\n",
        "Out-of-Bag (OOB) Score is an internal validation method in Random Forest and Bagging. Since each tree is trained on a bootstrap sample, some data points are left out. These unused samples are used to test the model, providing an unbiased estimate of performance without a separate validation set.\n",
        "\n",
        "5. How can you measure the importance of features in a Random Forest model?\n",
        "\n",
        "Feature importance in Random Forest is measured by calculating how much each feature decreases impurity across all trees. Features that contribute more to reducing impurity are assigned higher importance scores.\n",
        "\n",
        "6. Explain the working principle of a Bagging Classifier\n",
        "\n",
        "A Bagging Classifier works by creating multiple bootstrap samples from the dataset and training a base classifier on each sample. Predictions from all classifiers are combined using majority voting to produce the final output.\n",
        "\n",
        "7. How do you evaluate a Bagging Classifierâ€™s performance?\n",
        "\n",
        "Performance is evaluated using metrics such as accuracy, precision, recall, F1-score, ROC-AUC score, cross-validation, and OOB score if available.\n",
        "\n",
        "8. How does a Bagging Regressor work?\n",
        "\n",
        "A Bagging Regressor trains multiple regression models on different bootstrap samples. The final prediction is the average of predictions from all regressors, which reduces variance.\n",
        "\n",
        "9. What is the main advantage of ensemble techniques?\n",
        "\n",
        "The main advantage of ensemble techniques is improved predictive performance and robustness by combining multiple models instead of relying on a single model.\n",
        "\n",
        "10. What is the main challenge of ensemble methods?\n",
        "\n",
        "The main challenge is increased computational cost, complexity, and reduced interpretability compared to single models.\n",
        "\n",
        "11. Explain the key idea behind ensemble techniques\n",
        "\n",
        "The key idea is that combining multiple weak or diverse models leads to a stronger and more accurate overall model.\n",
        "\n",
        "12. What is a Random Forest Classifier?\n",
        "\n",
        "A Random Forest Classifier is an ensemble learning algorithm that builds multiple Decision Trees using bagging and feature randomness and combines their predictions using majority voting.\n",
        "\n",
        "13. What are the main types of ensemble techniques?\n",
        "\n",
        "The main types are Bagging, Boosting, and Stacking.\n",
        "\n",
        "14. What is ensemble learning in machine learning?\n",
        "\n",
        "Ensemble learning is a technique where multiple models are trained and combined to solve a problem more accurately than a single model.\n",
        "\n",
        "15. When should we avoid using ensemble methods?\n",
        "\n",
        "Ensemble methods should be avoided when computational resources are limited, model interpretability is crucial, or the dataset is very small.\n",
        "\n",
        "16. How does Bagging help in reducing overfitting?\n",
        "\n",
        "Bagging reduces overfitting by averaging predictions from multiple models trained on different data samples, which reduces variance.\n",
        "\n",
        "17. Why is Random Forest better than a single Decision Tree?\n",
        "\n",
        "Random Forest reduces overfitting, improves accuracy, and is more robust to noise compared to a single Decision Tree.\n",
        "\n",
        "18. What is the role of bootstrap sampling in Bagging?\n",
        "\n",
        "Bootstrap sampling creates multiple training datasets with replacement, ensuring diversity among models.\n",
        "\n",
        "19. What are some real-world applications of ensemble techniques?\n",
        "\n",
        "Applications include fraud detection, medical diagnosis, recommendation systems, stock market prediction, and image classification.\n",
        "\n",
        "20. What is the difference between Bagging and Boosting?\n",
        "\n",
        "Bagging trains models independently to reduce variance, while Boosting trains models sequentially, focusing on correcting previous errors to reduce bias."
      ],
      "metadata": {
        "id": "5uNAJgmwvQnq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PRACTICAL**"
      ],
      "metadata": {
        "id": "aOURMjj9vYYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#21\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "model = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEZT7r56vcj3",
        "outputId": "fe906a5d-8074-4509-9586-3f61e08c4400"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#22\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = load_diabetes(return_X_y=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "model = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrCARSBlv4Hk",
        "outputId": "b0f0a24c-660d-42d8-b68b-5a9b75aab902"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 2987.0073593984966\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#23\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf.fit(X, y)\n",
        "\n",
        "for name, importance in zip(data.feature_names, rf.feature_importances_):\n",
        "    print(name, \":\", importance)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoWapq2iv4ay",
        "outputId": "d5dc3fa7-7f2d-478e-f400-b43530c7deab"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean radius : 0.034843233980943286\n",
            "mean texture : 0.015225145712914773\n",
            "mean perimeter : 0.06799034063826767\n",
            "mean area : 0.0604616365111208\n",
            "mean smoothness : 0.0079584528113981\n",
            "mean compactness : 0.0115970382551153\n",
            "mean concavity : 0.06691736463414073\n",
            "mean concave points : 0.10704565721708294\n",
            "mean symmetry : 0.0034227883667066654\n",
            "mean fractal dimension : 0.002615076161734035\n",
            "radius error : 0.014263704023561991\n",
            "texture error : 0.003744265527721131\n",
            "perimeter error : 0.010085060356218195\n",
            "area error : 0.029552828963121246\n",
            "smoothness error : 0.0047215698751171715\n",
            "compactness error : 0.0056118342607874684\n",
            "concavity error : 0.005819693803295157\n",
            "concave points error : 0.0037597476696419278\n",
            "symmetry error : 0.003545970882211007\n",
            "fractal dimension error : 0.005942332118800315\n",
            "worst radius : 0.08284828183729644\n",
            "worst texture : 0.017485260960677165\n",
            "worst perimeter : 0.08084969717184524\n",
            "worst area : 0.13935694286788813\n",
            "worst smoothness : 0.012232023199117627\n",
            "worst compactness : 0.01986385650955019\n",
            "worst concavity : 0.03733871230020869\n",
            "worst concave points : 0.13222508566399135\n",
            "worst symmetry : 0.008179084261284875\n",
            "worst fractal dimension : 0.004497313458240299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#24\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "dt = DecisionTreeRegressor(random_state=42)\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "dt.fit(X_train, y_train)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Decision Tree MSE:\", mean_squared_error(y_test, dt.predict(X_test)))\n",
        "print(\"Random Forest MSE:\", mean_squared_error(y_test, rf.predict(X_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gfOFaV0v4gD",
        "outputId": "e605b11c-f99b-4ffe-ec24-7f22d2338f49"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree MSE: 5697.789473684211\n",
            "Random Forest MSE: 2859.641982706767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#25\n",
        "rf_oob = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    oob_score=True,\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf_oob.fit(X_train, y_train)\n",
        "\n",
        "print(\"OOB Score:\", rf_oob.oob_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAj0vZBLv4np",
        "outputId": "c030b065-0bec-4226-9417-128dd0ccabb0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOB Score: 0.006472491909385114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#26\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load binary classification dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Train-test split with stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.3,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "# Train Bagging Classifier with SVM\n",
        "model = BaggingClassifier(\n",
        "    estimator=SVC(kernel='rbf', probability=True),\n",
        "    n_estimators=10,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKNoFyXrv4sz",
        "outputId": "e1ee8a90-5574-4020-f880-7329f9c332b6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9181286549707602\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#27\n",
        "for n in [10, 50, 100]:\n",
        "    rf = RandomForestClassifier(n_estimators=n, random_state=42)\n",
        "    rf.fit(X_train, y_train)\n",
        "    acc = accuracy_score(y_test, rf.predict(X_test))\n",
        "    print(f\"Trees: {n}, Accuracy: {acc}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rA6u2cNCv4u9",
        "outputId": "f7dae9cf-1f65-418d-e8d5-b3aa0d3cca66"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trees: 10, Accuracy: 0.0\n",
            "Trees: 50, Accuracy: 0.007518796992481203\n",
            "Trees: 100, Accuracy: 0.007518796992481203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#28\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load binary classification dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Train-test split with stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.3,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "# Train Bagging Classifier with Logistic Regression\n",
        "model = BaggingClassifier(\n",
        "    estimator=LogisticRegression(max_iter=2000),\n",
        "    n_estimators=20,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for positive class\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute ROC-AUC score\n",
        "auc_score = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "print(\"AUC Score:\", auc_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHkW_pFYv4yT",
        "outputId": "4afe997b-9615-42fb-f060-2fc0e2622583"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC Score: 0.9932827102803738\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#29\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "misclassified = X_test[y_test != y_pred]\n",
        "print(\"Number of misclassified samples:\", len(misclassified))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLT5tvGTv42s",
        "outputId": "78a66adc-5132-48a1-8281-ffdc0821174e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of misclassified samples: 132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#30\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "bag = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bag.fit(X_train, y_train)\n",
        "\n",
        "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, dt.predict(X_test)))\n",
        "print(\"Bagging Accuracy:\", accuracy_score(y_test, bag.predict(X_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65iie3rvv47-",
        "outputId": "a78fc315-c1ac-4a09-e73b-446c81f7a034"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 0.022556390977443608\n",
            "Bagging Accuracy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#31\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_test, rf.predict(X_test))\n",
        "print(\"Confusion Matrix:\\n\", cm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2wejq3lv5Av",
        "outputId": "eccc8bfb-deb7-49fc-d9ed-ed0e470a541f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            " [[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#32\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, bag.predict(X_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F21hVtqVv5Fe",
        "outputId": "bc8277d2-fe9f-4807-ba76-ec29911d199b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "        25.0       0.00      0.00      0.00       0.0\n",
            "        37.0       0.00      0.00      0.00       1.0\n",
            "        39.0       0.00      0.00      0.00       0.0\n",
            "        42.0       0.00      0.00      0.00       1.0\n",
            "        43.0       0.00      0.00      0.00       0.0\n",
            "        44.0       0.00      0.00      0.00       0.0\n",
            "        48.0       0.00      0.00      0.00       2.0\n",
            "        49.0       0.00      0.00      0.00       0.0\n",
            "        51.0       0.00      0.00      0.00       0.0\n",
            "        52.0       0.00      0.00      0.00       3.0\n",
            "        53.0       0.00      0.00      0.00       1.0\n",
            "        55.0       0.00      0.00      0.00       0.0\n",
            "        58.0       0.00      0.00      0.00       0.0\n",
            "        59.0       0.00      0.00      0.00       1.0\n",
            "        60.0       0.00      0.00      0.00       2.0\n",
            "        61.0       0.00      0.00      0.00       2.0\n",
            "        63.0       0.00      0.00      0.00       3.0\n",
            "        64.0       0.00      0.00      0.00       1.0\n",
            "        65.0       0.00      0.00      0.00       1.0\n",
            "        66.0       0.00      0.00      0.00       0.0\n",
            "        67.0       0.00      0.00      0.00       1.0\n",
            "        68.0       0.00      0.00      0.00       1.0\n",
            "        69.0       0.00      0.00      0.00       1.0\n",
            "        70.0       0.00      0.00      0.00       1.0\n",
            "        71.0       0.00      0.00      0.00       1.0\n",
            "        72.0       0.00      0.00      0.00       4.0\n",
            "        73.0       0.00      0.00      0.00       0.0\n",
            "        74.0       0.00      0.00      0.00       0.0\n",
            "        75.0       0.00      0.00      0.00       0.0\n",
            "        77.0       0.00      0.00      0.00       1.0\n",
            "        78.0       0.00      0.00      0.00       0.0\n",
            "        81.0       0.00      0.00      0.00       0.0\n",
            "        84.0       0.00      0.00      0.00       2.0\n",
            "        85.0       0.00      0.00      0.00       0.0\n",
            "        87.0       0.00      0.00      0.00       1.0\n",
            "        88.0       0.00      0.00      0.00       1.0\n",
            "        89.0       0.00      0.00      0.00       1.0\n",
            "        90.0       0.00      0.00      0.00       4.0\n",
            "        91.0       0.00      0.00      0.00       1.0\n",
            "        93.0       0.00      0.00      0.00       1.0\n",
            "        94.0       0.00      0.00      0.00       2.0\n",
            "        95.0       0.00      0.00      0.00       1.0\n",
            "        96.0       0.00      0.00      0.00       2.0\n",
            "        97.0       0.00      0.00      0.00       2.0\n",
            "        98.0       0.00      0.00      0.00       1.0\n",
            "        99.0       0.00      0.00      0.00       1.0\n",
            "       101.0       0.00      0.00      0.00       2.0\n",
            "       102.0       0.00      0.00      0.00       2.0\n",
            "       104.0       0.00      0.00      0.00       1.0\n",
            "       107.0       0.00      0.00      0.00       1.0\n",
            "       108.0       0.00      0.00      0.00       1.0\n",
            "       109.0       0.00      0.00      0.00       0.0\n",
            "       110.0       0.00      0.00      0.00       1.0\n",
            "       111.0       0.00      0.00      0.00       1.0\n",
            "       113.0       0.00      0.00      0.00       1.0\n",
            "       114.0       0.00      0.00      0.00       0.0\n",
            "       118.0       0.00      0.00      0.00       1.0\n",
            "       121.0       0.00      0.00      0.00       1.0\n",
            "       122.0       0.00      0.00      0.00       1.0\n",
            "       123.0       0.00      0.00      0.00       0.0\n",
            "       124.0       0.00      0.00      0.00       0.0\n",
            "       125.0       0.00      0.00      0.00       1.0\n",
            "       126.0       0.00      0.00      0.00       0.0\n",
            "       127.0       0.00      0.00      0.00       0.0\n",
            "       128.0       0.00      0.00      0.00       2.0\n",
            "       129.0       0.00      0.00      0.00       2.0\n",
            "       132.0       0.00      0.00      0.00       1.0\n",
            "       135.0       0.00      0.00      0.00       1.0\n",
            "       136.0       0.00      0.00      0.00       1.0\n",
            "       137.0       0.00      0.00      0.00       1.0\n",
            "       138.0       0.00      0.00      0.00       0.0\n",
            "       139.0       0.00      0.00      0.00       0.0\n",
            "       140.0       0.00      0.00      0.00       2.0\n",
            "       141.0       0.00      0.00      0.00       0.0\n",
            "       143.0       0.00      0.00      0.00       0.0\n",
            "       144.0       0.00      0.00      0.00       1.0\n",
            "       147.0       0.00      0.00      0.00       0.0\n",
            "       150.0       0.00      0.00      0.00       1.0\n",
            "       151.0       0.00      0.00      0.00       1.0\n",
            "       153.0       0.00      0.00      0.00       1.0\n",
            "       156.0       0.00      0.00      0.00       1.0\n",
            "       158.0       0.00      0.00      0.00       1.0\n",
            "       160.0       0.00      0.00      0.00       0.0\n",
            "       162.0       0.00      0.00      0.00       0.0\n",
            "       163.0       0.00      0.00      0.00       0.0\n",
            "       164.0       0.00      0.00      0.00       1.0\n",
            "       166.0       0.00      0.00      0.00       1.0\n",
            "       167.0       0.00      0.00      0.00       1.0\n",
            "       168.0       0.00      0.00      0.00       2.0\n",
            "       170.0       0.00      0.00      0.00       1.0\n",
            "       171.0       0.00      0.00      0.00       1.0\n",
            "       172.0       0.00      0.00      0.00       1.0\n",
            "       173.0       0.00      0.00      0.00       1.0\n",
            "       174.0       0.00      0.00      0.00       1.0\n",
            "       178.0       0.00      0.00      0.00       1.0\n",
            "       179.0       0.00      0.00      0.00       0.0\n",
            "       180.0       0.00      0.00      0.00       1.0\n",
            "       181.0       0.00      0.00      0.00       1.0\n",
            "       182.0       0.00      0.00      0.00       0.0\n",
            "       184.0       0.00      0.00      0.00       1.0\n",
            "       186.0       0.00      0.00      0.00       1.0\n",
            "       187.0       0.00      0.00      0.00       1.0\n",
            "       189.0       0.00      0.00      0.00       1.0\n",
            "       190.0       0.00      0.00      0.00       1.0\n",
            "       191.0       0.00      0.00      0.00       0.0\n",
            "       195.0       0.00      0.00      0.00       0.0\n",
            "       197.0       0.00      0.00      0.00       1.0\n",
            "       200.0       0.00      0.00      0.00       1.0\n",
            "       201.0       0.00      0.00      0.00       0.0\n",
            "       202.0       0.00      0.00      0.00       2.0\n",
            "       206.0       0.00      0.00      0.00       1.0\n",
            "       214.0       0.00      0.00      0.00       1.0\n",
            "       219.0       0.00      0.00      0.00       1.0\n",
            "       220.0       0.00      0.00      0.00       2.0\n",
            "       222.0       0.00      0.00      0.00       1.0\n",
            "       229.0       0.00      0.00      0.00       0.0\n",
            "       230.0       0.00      0.00      0.00       1.0\n",
            "       232.0       0.00      0.00      0.00       2.0\n",
            "       233.0       0.00      0.00      0.00       2.0\n",
            "       235.0       0.00      0.00      0.00       0.0\n",
            "       236.0       0.00      0.00      0.00       0.0\n",
            "       237.0       0.00      0.00      0.00       1.0\n",
            "       242.0       0.00      0.00      0.00       2.0\n",
            "       248.0       0.00      0.00      0.00       1.0\n",
            "       252.0       0.00      0.00      0.00       1.0\n",
            "       257.0       0.00      0.00      0.00       1.0\n",
            "       258.0       0.00      0.00      0.00       1.0\n",
            "       259.0       0.00      0.00      0.00       0.0\n",
            "       262.0       0.00      0.00      0.00       1.0\n",
            "       263.0       0.00      0.00      0.00       1.0\n",
            "       264.0       0.00      0.00      0.00       1.0\n",
            "       265.0       0.00      0.00      0.00       1.0\n",
            "       270.0       0.00      0.00      0.00       1.0\n",
            "       272.0       0.00      0.00      0.00       1.0\n",
            "       275.0       0.00      0.00      0.00       1.0\n",
            "       277.0       0.00      0.00      0.00       1.0\n",
            "       279.0       0.00      0.00      0.00       0.0\n",
            "       280.0       0.00      0.00      0.00       1.0\n",
            "       281.0       0.00      0.00      0.00       2.0\n",
            "       283.0       0.00      0.00      0.00       1.0\n",
            "       292.0       0.00      0.00      0.00       0.0\n",
            "       295.0       0.00      0.00      0.00       1.0\n",
            "       297.0       0.00      0.00      0.00       1.0\n",
            "       310.0       0.00      0.00      0.00       1.0\n",
            "       311.0       0.00      0.00      0.00       0.0\n",
            "       332.0       0.00      0.00      0.00       0.0\n",
            "       341.0       0.00      0.00      0.00       0.0\n",
            "\n",
            "    accuracy                           0.00     133.0\n",
            "   macro avg       0.00      0.00      0.00     133.0\n",
            "weighted avg       0.00      0.00      0.00     133.0\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#33\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(bag, X, y, cv=5)\n",
        "print(\"Cross-validation Accuracy:\", scores.mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnVBwAC3v5Kk",
        "outputId": "3d6c40e3-8fd0-4ea5-8fd6-dc816679e096"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation Accuracy: 0.9543859649122808\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#34\n",
        "for depth in [2, 5, 10]:\n",
        "    rf = RandomForestClassifier(max_depth=depth, random_state=42)\n",
        "    rf.fit(X_train, y_train)\n",
        "    acc = accuracy_score(y_test, rf.predict(X_test))\n",
        "    print(f\"Max Depth: {depth}, Accuracy: {acc}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5JKDMT7v5PW",
        "outputId": "4cc8bf70-e2ee-4746-e930-5850fee63eac"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Depth: 2, Accuracy: 0.0\n",
            "Max Depth: 5, Accuracy: 0.007518796992481203\n",
            "Max Depth: 10, Accuracy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#35\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "bag_dt = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bag_knn = BaggingRegressor(\n",
        "    estimator=KNeighborsRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bag_dt.fit(X_train, y_train)\n",
        "bag_knn.fit(X_train, y_train)\n",
        "\n",
        "print(\"DT Bagging MSE:\", mean_squared_error(y_test, bag_dt.predict(X_test)))\n",
        "print(\"KNN Bagging MSE:\", mean_squared_error(y_test, bag_knn.predict(X_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vViyX0gTv5UE",
        "outputId": "3b531e64-6147-4b0c-d9b5-f07ca1c359f3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DT Bagging MSE: 2987.0073593984966\n",
            "KNN Bagging MSE: 3140.186131007519\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#36\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "estimators = [\n",
        "    ('dt', DecisionTreeClassifier(random_state=42)),\n",
        "    ('svm', SVC(probability=True, random_state=42)),\n",
        "    ('lr', LogisticRegression(max_iter=2000))\n",
        "]\n",
        "\n",
        "stack = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=LogisticRegression()\n",
        ")\n",
        "\n",
        "stack.fit(X_train, y_train)\n",
        "y_pred = stack.predict(X_test)\n",
        "\n",
        "print(\"Stacking Classifier Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsqUshsVv5Yf",
        "outputId": "e857c8c2-31e2-4d40-805c-eaefc961420f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking Classifier Accuracy: 0.9707602339181286\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#37\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "importances = rf.feature_importances_\n",
        "indices = np.argsort(importances)[-5:]\n",
        "\n",
        "print(\"Top 5 Important Features:\")\n",
        "for i in indices:\n",
        "    print(data.feature_names[i], \":\", importances[i])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIE1kausv5di",
        "outputId": "22f8e23d-7180-446c-f514-83e2337f8aa7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Important Features:\n",
            "worst perimeter : 0.08084969717184524\n",
            "worst radius : 0.08284828183729644\n",
            "mean concave points : 0.10704565721708294\n",
            "worst concave points : 0.13222508566399135\n",
            "worst area : 0.13935694286788813\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#38\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "bag = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bag.fit(X_train, y_train)\n",
        "y_pred = bag.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fn-SjLxwv5he",
        "outputId": "f88eca70-85c7-47e4-8ca8-c93571e5546b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.94      0.94        63\n",
            "           1       0.96      0.97      0.97       108\n",
            "\n",
            "    accuracy                           0.96       171\n",
            "   macro avg       0.96      0.95      0.96       171\n",
            "weighted avg       0.96      0.96      0.96       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#39\n",
        "for depth in [2, 5, 10, None]:\n",
        "    rf = RandomForestClassifier(\n",
        "        max_depth=depth,\n",
        "        n_estimators=100,\n",
        "        random_state=42\n",
        "    )\n",
        "    rf.fit(X_train, y_train)\n",
        "    acc = accuracy_score(y_test, rf.predict(X_test))\n",
        "    print(\"Max Depth:\", depth, \"Accuracy:\", acc)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACudlkTvwNte",
        "outputId": "a0eca6b6-7d3e-4f83-fae9-1f17f5eca710"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Depth: 2 Accuracy: 0.0\n",
            "Max Depth: 5 Accuracy: 0.007518796992481203\n",
            "Max Depth: 10 Accuracy: 0.0\n",
            "Max Depth: None Accuracy: 0.007518796992481203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#40\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X, y = load_diabetes(return_X_y=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "bag_dt = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bag_knn = BaggingRegressor(\n",
        "    estimator=KNeighborsRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bag_dt.fit(X_train, y_train)\n",
        "bag_knn.fit(X_train, y_train)\n",
        "\n",
        "print(\"Decision Tree Bagging MSE:\",\n",
        "      mean_squared_error(y_test, bag_dt.predict(X_test)))\n",
        "\n",
        "print(\"KNN Bagging MSE:\",\n",
        "      mean_squared_error(y_test, bag_knn.predict(X_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQnCA9SiwOIX",
        "outputId": "0317f0b9-731c-42c2-b1d9-579e2d7b02eb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Bagging MSE: 2987.0073593984966\n",
            "KNN Bagging MSE: 3140.186131007519\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#41\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load a binary classification dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Train-test split with stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.3,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "# Train Random Forest model\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for positive class\n",
        "y_prob = rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "print(\"ROC-AUC Score:\", roc_auc)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hU7EhY-wOUs",
        "outputId": "90c85dc6-7e36-45f7-dd39-e4c555abbad4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.991311331775701\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#42\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "bag = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "scores = cross_val_score(bag, X, y, cv=5)\n",
        "print(\"Cross-Validation Accuracy:\", scores.mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cbwywc7IwSbS",
        "outputId": "94329ca7-7a0d-4328-c536-4a22a49434df"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Validation Accuracy: 0.009065372829417773\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#43\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load binary dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train model\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_prob = rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Precision-Recall Curve\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
        "\n",
        "plt.plot(recall, precision)\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "lqjA98MSwStX",
        "outputId": "179a6ea7-ab45-4562-9f9a-c03586c74b99"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#44\n",
        "estimators = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "    ('lr', LogisticRegression(max_iter=2000))\n",
        "]\n",
        "\n",
        "stack = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=LogisticRegression()\n",
        ")\n",
        "\n",
        "stack.fit(X_train, y_train)\n",
        "y_pred = stack.predict(X_test)\n",
        "\n",
        "print(\"Stacking (RF + LR) Accuracy:\",\n",
        "      accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa3AgsnWwS4I",
        "outputId": "34c4981c-9f06-490a-c3fb-66e0378d45a3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py:1380: RuntimeWarning: Number of classes in training fold (175) does not match total number of classes (192). Results may not be appropriate for your use case. To fix this, use a cross-validation technique resulting in properly stratified folds\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py:1380: RuntimeWarning: Number of classes in training fold (164) does not match total number of classes (192). Results may not be appropriate for your use case. To fix this, use a cross-validation technique resulting in properly stratified folds\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py:1380: RuntimeWarning: Number of classes in training fold (168) does not match total number of classes (192). Results may not be appropriate for your use case. To fix this, use a cross-validation technique resulting in properly stratified folds\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py:1380: RuntimeWarning: Number of classes in training fold (168) does not match total number of classes (192). Results may not be appropriate for your use case. To fix this, use a cross-validation technique resulting in properly stratified folds\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py:1380: RuntimeWarning: Number of classes in training fold (174) does not match total number of classes (192). Results may not be appropriate for your use case. To fix this, use a cross-validation technique resulting in properly stratified folds\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py:1380: RuntimeWarning: Number of classes in training fold (175) does not match total number of classes (192). Results may not be appropriate for your use case. To fix this, use a cross-validation technique resulting in properly stratified folds\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py:1380: RuntimeWarning: Number of classes in training fold (164) does not match total number of classes (192). Results may not be appropriate for your use case. To fix this, use a cross-validation technique resulting in properly stratified folds\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py:1380: RuntimeWarning: Number of classes in training fold (168) does not match total number of classes (192). Results may not be appropriate for your use case. To fix this, use a cross-validation technique resulting in properly stratified folds\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py:1380: RuntimeWarning: Number of classes in training fold (168) does not match total number of classes (192). Results may not be appropriate for your use case. To fix this, use a cross-validation technique resulting in properly stratified folds\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py:1380: RuntimeWarning: Number of classes in training fold (174) does not match total number of classes (192). Results may not be appropriate for your use case. To fix this, use a cross-validation technique resulting in properly stratified folds\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking (RF + LR) Accuracy: 0.007518796992481203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#45\n",
        "for bootstrap in [True, False]:\n",
        "    bag = BaggingRegressor(\n",
        "        estimator=DecisionTreeRegressor(),\n",
        "        n_estimators=50,\n",
        "        bootstrap=bootstrap,\n",
        "        random_state=42\n",
        "    )\n",
        "    bag.fit(X_train, y_train)\n",
        "    mse = mean_squared_error(y_test, bag.predict(X_test))\n",
        "    print(\"Bootstrap:\", bootstrap, \"MSE:\", mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5SLsTkzwTCI",
        "outputId": "46990711-c2e0-466a-d92c-b1729a376738"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrap: True MSE: 2987.0073593984966\n",
            "Bootstrap: False MSE: 5190.679618045113\n"
          ]
        }
      ]
    }
  ]
}